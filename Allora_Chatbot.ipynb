{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ead1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ì‘ë‹µ: role='assistant' content=' ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Googleì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. \\n\\në„¤, ì €ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ˜Š  ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? \\n\\n\\n**(Korean: ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Googleì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ë„¤, ì €ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?)**\\n' images=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma2:2b\",\n",
    "    # model=\"qwen2.5:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”! ì´ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë‚˜ìš”?\"}]\n",
    ")\n",
    "\n",
    "print(\"ğŸ§  ëª¨ë¸ ì‘ë‹µ:\", response['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451c8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 984ê°œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“Œ ì´ 984 ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# âœ… JSON ë°ì´í„° ë¡œë“œ\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# âœ… info í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_data(folder_path=\"info\"):\n",
    "    data = []\n",
    "    json_files = glob(os.path.join(folder_path, \"*.json\"))  # í´ë” ë‚´ ëª¨ë“  .json íŒŒì¼ ì°¾ê¸°\n",
    "\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                file_data = json.load(f)\n",
    "                if isinstance(file_data, list):\n",
    "                    data.extend(file_data)  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€\n",
    "                else:\n",
    "                    print(f\"âš ï¸ {file_path}ì˜ ë°ì´í„° í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜. ë¬´ì‹œë¨.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ JSON íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {e}\")\n",
    "\n",
    "    print(f\"âœ… ì´ {len(data)}ê°œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return data\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¡œë“œ ì‹¤í–‰\n",
    "all_data = load_data(\"info\")\n",
    "\n",
    "# âœ… ë°ì´í„° í™•ì¸\n",
    "print(f\"ğŸ“Œ ì´ {len(all_data)} ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfc268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaDB ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì ìš© ì™„ë£Œ! í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì œê±° ì™„ë£Œ!\n",
      "ğŸ“Œ ì „ì£¼ëŒ€ ì¥í•™ê¸ˆ ì •ë³´: ì „ì£¼ëŒ€í•™êµì—ì„œëŠ” ë‹¤ì–‘í•œ êµë‚´ ì¥í•™ê¸ˆì„ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤.... (Score: 11.251113963947407)\n",
      "ğŸ“Œ êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ì•ˆë‚´: êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ê¸°ê°„ì€ 2ì›” 4ì¼ë¶€í„° 3ì›” 18ì¼ê¹Œì§€ì…ë‹ˆë‹¤.... (Score: 11.794244487222851)\n",
      "ğŸ“Œ ì„±ì ìš°ìˆ˜ ì¥í•™ê¸ˆ: ì§ì „ í•™ê¸° í‰ì  4.0 ì´ìƒ í•™ìƒë“¤ì—ê²Œ ì§€ê¸‰ë©ë‹ˆë‹¤.... (Score: 13.803185207375408)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\")\n",
    "\n",
    "# âœ… Sentence Transformer ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# âœ… JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ ë°ì´í„° ìƒì„± í›„ ì €ì¥\n",
    "json_file_path = \"processed_data.json\"\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(\"âš ï¸ 'processed_data.json' íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    text_data = [\n",
    "        {\"title\": \"êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ì•ˆë‚´\", \"content\": \"êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ê¸°ê°„ì€ 2ì›” 4ì¼ë¶€í„° 3ì›” 18ì¼ê¹Œì§€ì…ë‹ˆë‹¤.\", \"url\": \"https://www.kosaf.go.kr\"},\n",
    "        {\"title\": \"ì„±ì ìš°ìˆ˜ ì¥í•™ê¸ˆ\", \"content\": \"ì§ì „ í•™ê¸° í‰ì  4.0 ì´ìƒ í•™ìƒë“¤ì—ê²Œ ì§€ê¸‰ë©ë‹ˆë‹¤.\", \"url\": \"https://www.university.com/scholarship\"},\n",
    "        {\"title\": \"ì „ì£¼ëŒ€ ì¥í•™ê¸ˆ ì •ë³´\", \"content\": \"ì „ì£¼ëŒ€í•™êµì—ì„œëŠ” ë‹¤ì–‘í•œ êµë‚´ ì¥í•™ê¸ˆì„ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤.\", \"url\": \"https://www.jj.ac.kr\"},\n",
    "    ]\n",
    "\n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(text_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… {json_file_path} íŒŒì¼ì„ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# âœ… JSON ë°ì´í„° ë¡œë“œ\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = json.load(f)\n",
    "\n",
    "# âœ… ChromaDBì— ë°ì´í„° ì¶”ê°€\n",
    "if collection.count() == 0:\n",
    "    print(\"âš ï¸ ChromaDBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‚½ì…í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    for item in text_data:\n",
    "        doc_id = item.get(\"url\", item[\"title\"])  # ë¬¸ì„œì˜ URL ë˜ëŠ” ì œëª©ì„ IDë¡œ ì‚¬ìš©\n",
    "        collection.add(\n",
    "            ids=[doc_id],  # ìœ ë‹ˆí¬ ID\n",
    "            documents=[item[\"content\"]],  # âœ… content(ë¬¸ì„œ ë³¸ë¬¸)ë§Œ ì „ë‹¬\n",
    "            metadatas=[{\"title\": item[\"title\"], \"url\": item.get(\"url\", \"URL ì—†ìŒ\")}],  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°\n",
    "            embeddings=[embedding_model.encode(item[\"content\"]).tolist()]  # ë¬¸ì„œ ë‚´ìš© ì„ë² ë”©\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… {len(text_data)}ê°œì˜ ë°ì´í„°ë¥¼ ChromaDBì— ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… ChromaDB ê²€ìƒ‰ í•¨ìˆ˜ (FAISS ì œê±° ì™„ë£Œ, í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì œê±°)\n",
    "def search_data(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode(query).tolist()  # ì§ˆë¬¸ ë²¡í„°í™”\n",
    "\n",
    "    # ChromaDBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    retrieved_data = []\n",
    "    if results[\"documents\"]:  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°\n",
    "        for i, doc_content in enumerate(results[\"documents\"][0]):  # ğŸ”¥ doc_contentëŠ” ë‹¨ìˆœ ë¬¸ìì—´\n",
    "            retrieved_data.append({\n",
    "                \"title\": results[\"metadatas\"][0][i][\"title\"],  # ğŸ”¥ titleì„ metadatasì—ì„œ ê°€ì ¸ì˜´\n",
    "                \"content\": doc_content,  # ğŸ”¥ contentëŠ” ChromaDBì—ì„œ ë°˜í™˜ëœ ë¬¸ìì—´\n",
    "                \"url\": results[\"metadatas\"][0][i].get(\"url\", \"URL ì—†ìŒ\"),  # ğŸ”¥ urlë„ metadatasì—ì„œ ê°€ì ¸ì˜´\n",
    "                \"score\": results[\"distances\"][0][i]  # ChromaDBì—ì„œëŠ” ê±°ë¦¬ ê¸°ë°˜ ì •ë ¬\n",
    "            })\n",
    "\n",
    "    if not retrieved_data:\n",
    "        print(\"âš ï¸ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "\n",
    "    # âœ… ì ìˆ˜ê°€ ë‚®ì€(ìš°ì„ ìˆœìœ„ ë†’ì€) ìˆœì„œë¡œ ì •ë ¬ (í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì œê±°)\n",
    "    retrieved_data = sorted(retrieved_data, key=lambda x: x[\"score\"])\n",
    "\n",
    "    return retrieved_data\n",
    "\n",
    "print(\"âœ… ChromaDB ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì ìš© ì™„ë£Œ! í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì œê±° ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "query = \"êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ë°©ë²•\"\n",
    "answer = search_data(query)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for res in answer:\n",
    "    print(f\"ğŸ“Œ {res['title']}: {res['content'][:300]}... (Score: {res['score']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a17640",
   "metadata": {},
   "source": [
    "### ğŸ”„ï¸ Allora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003b88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "Could not create share link. Missing file: C:\\Users\\juneo\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\frpc_windows_arm64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_windows_arm64.exe\n",
      "2. Rename the downloaded file to: frpc_windows_arm64_v0.3\n",
      "3. Move the file to this location: C:\\Users\\juneo\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# âœ… ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# âœ… ì •í™•í•œ ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ê°œì„ ëœ í•¨ìˆ˜\n",
    "def detect_language(text):\n",
    "    \"\"\" langdetectë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ê°ì§€ \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang  # 'ko' ë˜ëŠ” 'en' ë°˜í™˜\n",
    "    except:\n",
    "        return \"ko\"  # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´\n",
    "\n",
    "# âœ… í…ìŠ¤íŠ¸ ë²ˆì—­ í•¨ìˆ˜ (ì¶”í›„ í™•ì¥ ê°€ëŠ¥)\n",
    "def translate_text(text, target_language=\"en\"):\n",
    "    \"\"\" ë²ˆì—­ API ì—°ê²° (ì¶”í›„ í™•ì¥ ê°€ëŠ¥) \"\"\"\n",
    "    return text  # í˜„ì¬ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì§€ë§Œ, ì‹¤ì œ ë²ˆì—­ API ì—°ê²° ê°€ëŠ¥\n",
    "\n",
    "# âœ… ê° í•™êµëª…ì— í•´ë‹¹í•˜ëŠ” JSON íŒŒì¼ ë§¤í•‘\n",
    "school_json_map = {\n",
    "    \"ì˜ì§„ì „ë¬¸ëŒ€\": \"./info/ì˜ì§„ì „ë¬¸ëŒ€_ìµœì¢….json\",\n",
    "    \"Yeungjin College\": \"./info/ì˜ì§„ì „ë¬¸ëŒ€_ìµœì¢….json\",\n",
    "    \"YJU\": \"./info/ì˜ì§„ì „ë¬¸ëŒ€_ìµœì¢….json\",\n",
    "    \"ê²½ë¶ëŒ€\": \"./info/ê²½ë¶ëŒ€_ìµœì¢….json\",\n",
    "    \"Kyungpook University\": \"./info/ê²½ë¶ëŒ€_ìµœì¢….json\",\n",
    "    \"KNU\": \"./info/ê²½ë¶ëŒ€_ìµœì¢….json\",\n",
    "    \"ì „ì£¼ëŒ€\": \"./info/ì „ì£¼ëŒ€_ìµœì¢….json\",\n",
    "    \"Jeonju University\": \"./info/ì „ì£¼ëŒ€_ìµœì¢….json\",\n",
    "    \"JJU\": \"./info/ì „ì£¼ëŒ€_ìµœì¢….json\",\n",
    "    \"ì„œìš¸ì‹œë¦½ëŒ€\": \"./info/ì‹œë¦½ëŒ€_ìµœì¢….json\",\n",
    "    \"UOS\": \"./info/ì‹œë¦½ëŒ€_ìµœì¢….json\",\n",
    "    \"ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€\": \"./info/ê³¼ê¸°ëŒ€_ìµœì¢….json\",\n",
    "    \"SNUST\": \"./info/ê³¼ê¸°ëŒ€_ìµœì¢….json\",\n",
    "    \"ì „ë‚¨ëŒ€\": \"./info/ì „ë‚¨ëŒ€_ìµœì¢….json\",\n",
    "    \"Chonnam University\": \"./info/ì „ë‚¨ëŒ€_ìµœì¢….json\",\n",
    "    \"JNU\": \"./info/ì „ë‚¨ëŒ€_ìµœì¢….json\",\n",
    "    \"CNU\": \"./info/ì „ë‚¨ëŒ€_ìµœì¢….json\",\n",
    "    \"ì„±ê· ê´€ëŒ€\": \"./info/ì„±ê· ê´€ëŒ€_ìµœì¢….json\",\n",
    "    \"ì„±ëŒ€\": \"./info/ì„±ê· ê´€ëŒ€_ìµœì¢….json\",\n",
    "    \"ì„±ê· ê´€\": \"./info/ì„±ê· ê´€ëŒ€_ìµœì¢….json\",\n",
    "    \"Sungkyunkwan University\": \"./info/ì„±ê· ê´€ëŒ€_ìµœì¢….json\",\n",
    "    \"SKKU\": \"./info/ì„±ê· ê´€ëŒ€_ìµœì¢….json\",\n",
    "}\n",
    "\n",
    "def detect_school(query):\n",
    "    \"\"\" ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•™êµëª…ì„ ê°ì§€ \"\"\"\n",
    "    for school in school_json_map.keys():\n",
    "        if school in query:\n",
    "            return school\n",
    "    return None  # í•´ë‹¹í•˜ëŠ” í•™êµ ì—†ìŒ\n",
    "\n",
    "def load_data(school):\n",
    "    \"\"\" íŠ¹ì • í•™êµì˜ JSON ë°ì´í„°ë§Œ ë¡œë“œ \"\"\"\n",
    "    json_file = school_json_map.get(school)\n",
    "    if not json_file:\n",
    "        return []\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {json_file}, {e}\")\n",
    "        return []\n",
    "\n",
    "def search_data(query, school):\n",
    "    \"\"\" ë¬¸ì¥ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Sentence Embedding) \"\"\"\n",
    "    all_data = load_data(school)\n",
    "    if not all_data:\n",
    "        return []\n",
    "\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    scores = []\n",
    "    for item in all_data:\n",
    "        doc_text = f\"{item['title']} {item['content']}\"\n",
    "        doc_embedding = embedding_model.encode(doc_text, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, doc_embedding).item()\n",
    "\n",
    "        if similarity > 0.5:  # ìœ ì‚¬ë„ê°€ 50% ì´ìƒì¼ ë•Œë§Œ í¬í•¨\n",
    "            scores.append((similarity, item))\n",
    "\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    return [res[1] for res in scores[:5]]  # ìµœëŒ€ 5ê°œ ê²°ê³¼ ë°˜í™˜\n",
    "\n",
    "def clean_response(response_text):\n",
    "    \"\"\" ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ \"\"\"\n",
    "\n",
    "    # âœ… \"role='assistant' content=\" ì œê±°\n",
    "    response_text = re.sub(r\"role=['\\\"]assistant['\\\"] content=['\\\"]\", \"\", response_text)\n",
    "\n",
    "    # âœ… \"\\n\" (ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì), \"\\r\" ì œê±°\n",
    "    # response_text = response_text.replace(\"\\\\n\", \"     \").replace(\"\\r\", \" \")\n",
    "\n",
    "    # âœ… ìœ ë‹ˆì½”ë“œ íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ì˜ˆ: \\u200d)\n",
    "    response_text = re.sub(r\"\\u200d\", \"\", response_text)\n",
    "\n",
    "    # âœ… \"**ì˜ì–´:**\", \"**Korean:**\" ì œê±°\n",
    "    response_text = re.sub(r\"\\*\\*ì˜ì–´:\\*\\*|\\*\\*Korean:\\*\\*\", \"\", response_text)\n",
    "\n",
    "    # âœ… \"images=None tool_calls=None\" ì œê±°\n",
    "    response_text = re.sub(r\"images=None tool_calls=None\", \"\", response_text)\n",
    "\n",
    "    # âœ… ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ë¦¬\n",
    "    response_text = re.sub(r\"\\s+\", \" \", response_text)\n",
    "\n",
    "    # âœ… ì•ë’¤ ê³µë°± ì •ë¦¬\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "def generate_response(query):\n",
    "    \"\"\" ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , í•´ë‹¹ í•™êµ JSON ë°ì´í„°ì—ì„œ ì‘ë‹µ ìƒì„± \"\"\"\n",
    "    detected_lang = detect_language(query)  # âœ… ê°œì„ ëœ ì–¸ì–´ ê°ì§€ ì ìš©\n",
    "\n",
    "    school = detect_school(query)\n",
    "\n",
    "    if not school:\n",
    "        return \"âš ï¸ í•™êµëª…ì„ ì •í™•í•˜ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ) 'ì‹œë¦½ëŒ€ ì„±ì  ì¥í•™ê¸ˆ ì•Œë ¤ì¤˜'\"\n",
    "\n",
    "    search_results = search_data(query, school)\n",
    "\n",
    "    if not search_results:\n",
    "        return f\"âš ï¸ {school} ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”!\"\n",
    "\n",
    "    seen_urls = set()\n",
    "    filtered_context = []\n",
    "    for res in search_results:\n",
    "        url = res.get('url', 'URL ì—†ìŒ')\n",
    "        if url not in seen_urls:\n",
    "            filtered_context.append(f\"ğŸ“Œ {res['title']} (URL: {url})\\n  {res['content'][:200]}...\")\n",
    "            seen_urls.add(url)\n",
    "\n",
    "    context = \"\\n\".join(filtered_context)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ì‚¬ìš©ìì˜ ì§ˆë¬¸: \"{query}\"\n",
    "\n",
    "    - ì•„ë˜ëŠ” {school} ê´€ë ¨ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ ì™¸ì— ì¶”ê°€ì ì¸ ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    - ëŒ€ë‹µì—ëŠ” í•´ë‹¹ ì •ë³´ì˜ urlì„ ë§í¬í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ í•œë²ˆì”©ì€ ê¼­ ì œê³µí•´ì£¼ì„¸ìš”!\n",
    "    - ê·¸ë¦¬ê³  ë¬¸ë‹¨, ë¬¸ì¥, ë‹µë³€ì„ ì •ë¦¬í• ë•Œ ì´ëª¨í‹°ì½˜ì„ ìµœëŒ€í•œ ë§ì´ í™œìš©í•´ì£¼ì„¸ìš”.\n",
    "    - **ë°˜ë“œì‹œ ì§ˆë¬¸ê³¼ ë™ì¼í•œ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”!** \n",
    "    - **ì˜ˆì™¸ ì—†ì´ ì§ˆë¬¸ì´ ì˜ì–´ë©´ ì˜ì–´ë¡œ, í•œêµ­ì–´ë©´ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.**\n",
    "\n",
    "    ë‹µë³€ì—ëŠ” json íŒŒì¼ì— ìˆëŠ” ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•´ì£¼ì„¸ìš”. content ë¶€ë¶„ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ë‹µë³€ì— í¬í•¨í•´ì£¼ì„¸ìš”.\n",
    "    ê²€ìƒ‰ëœ ì •ë³´:\n",
    "    {context}\n",
    "\n",
    "    ìœ„ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ ,ë¶„ì„í•˜ì—¬ ìµœëŒ€í•œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"gemma2:2b\",\n",
    "        # model=\"qwen2.5:1.5b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    chatbot_response = str(response['message'])\n",
    "\n",
    "    # âœ… ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "    chatbot_response = clean_response(chatbot_response)\n",
    "    response_lang = detect_language(chatbot_response)\n",
    "    if response_lang != detected_lang:\n",
    "        chatbot_response = translate_text(chatbot_response, target_language=detected_lang)\n",
    "\n",
    "    return chatbot_response\n",
    "\n",
    "# âœ… Gradio UI ìƒì„±\n",
    "def chat_interface(user_input):\n",
    "    return generate_response(user_input)\n",
    "\n",
    "# âœ… Gradio ì•± ì‹¤í–‰í–‰\n",
    "# with gr.Blocks() as app:\n",
    "#     gr.Markdown(\"# ğŸ“ ëŒ€í•™ ì¥í•™ê¸ˆ ì •ë³´ ì±—ë´‡ ğŸ’¬\")\n",
    "#     gr.Markdown(\"### ì›í•˜ëŠ” ëŒ€í•™êµì™€ ì¥í•™ê¸ˆ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!\")\n",
    "\n",
    "#     user_input = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì„±ê· ê´€ëŒ€ ì„±ì  ì¥í•™ê¸ˆ)\")\n",
    "#     output = gr.Textbox(label=\"ì±—ë´‡ì˜ ë‹µë³€\")\n",
    "\n",
    "#     submit_button = gr.Button(\"ê²€ìƒ‰í•˜ê¸°\")\n",
    "\n",
    "#     submit_button.click(fn=chat_interface, inputs=user_input, outputs=output)\n",
    "\n",
    "# app.launch(share=True)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def chat_interface(user_input):\n",
    "    response = generate_response(user_input)\n",
    "    response = response.replace(\"\\\\n\", \"<br>\")  # âœ… HTML ì¤„ë°”ê¿ˆ íƒœê·¸ ì ìš©\n",
    "    response = response.replace(\"####\", \"ğŸ’ \")  # âœ… HTML ì¤„ë°”ê¿ˆ íƒœê·¸ ì ìš©\n",
    "    response = response.replace(\"###\", \"ğŸ”¹\")  # âœ… HTML ì¤„ë°”ê¿ˆ íƒœê·¸ ì ìš©\n",
    "    response = response.replace(\"\\\\u200d\", \" \")  # âœ… HTML ì¤„ë°”ê¿ˆ íƒœê·¸ ì ìš©\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# ğŸ“ ëŒ€í•™ ì¥í•™ê¸ˆ ì •ë³´ ì±—ë´‡ ğŸ’¬\")\n",
    "    gr.Markdown(\"### ì›í•˜ëŠ” ëŒ€í•™êµì™€ ì¥í•™ê¸ˆ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!\")\n",
    "\n",
    "    user_input = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì„±ê· ê´€ëŒ€ ì„±ì  ì¥í•™ê¸ˆ)\")\n",
    "    output = gr.Markdown()  # âœ… Markdownì„ ì‚¬ìš©í•˜ì—¬ HTML ë Œë”ë§ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •\n",
    "\n",
    "    submit_button = gr.Button(\"ê²€ìƒ‰í•˜ê¸°\")\n",
    "\n",
    "    submit_button.click(fn=chat_interface, inputs=user_input, outputs=output)\n",
    "\n",
    "app.launch(share=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
